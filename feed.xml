<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Huailiang Blog</title>
    <description>关于前端与设计、黑客与画家 | 怀亮，Web &amp; Mobile Lover，Software Engineer，Game Designer | 这里是 @Huailiang怀亮 的个人博客，与你一起发现更大的世界。</description>
    <link>https://huailiang.github.io/</link>
    <atom:link href="https://huailiang.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 03 May 2019 11:55:14 +0800</pubDate>
    <lastBuildDate>Fri, 03 May 2019 11:55:14 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>手机游戏画面风格转换</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;如今人工智能大行其道， 其中在图像、影音处理中方法颇多。 本文介绍一种游戏中图像风格转换的例子，训练采用Tensorflow-GAN的方式，运行时在Unity引擎使用compute shader实现了跟tensorflow中一样的前向传播的网络来生成转换后的风格。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;训练集&quot;&gt;训练集&lt;/h2&gt;

&lt;p&gt;我们在python-tensorflow中实现了一种反向传播网络，我们使用Auto-Encoder替代GAN中Generator，用以生成风格化的图像， 而在Discrimator来鉴别图像。&lt;/p&gt;

&lt;p&gt;训练中采用的训练集是微软的coco的&lt;a href=&quot;http://mscoco.org&quot;&gt;dataset&lt;/a&gt;, 下载转换风格&lt;a href=&quot;https://hcicloud.iwr.uni-heidelberg.de/index.php/s/NcJj2oLBTYuT1tf&quot;&gt;图片集&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-tf/style1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们在训练Discrimator的时候， 评估损失函数，丢给Discrimator风格化的图片尽量大， coco训练集的图片因为是假的， 我们去使其输出的值尽量小， 通过gennerator的图片，也是假的，我们也使其值也越小。&lt;/p&gt;

&lt;p&gt;在训练Genenrator评估其损失函数的时候， generator的图片丢给Discrimator，尽量和真实的风格图片尽量接近，因此我们使其输出的值也要越大。&lt;/p&gt;

&lt;p&gt;这里定义content image的内容的损失，方法采用的如&lt;a href=&quot;https://arxiv.org/abs/1807.10201&quot;&gt;paper&lt;/a&gt; 提到maxpooling方式。&lt;/p&gt;

&lt;p&gt;定义style featur的损失， 方法采用把generator生成的图片和原始输入的style image做差求均值。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;
# Image loss.
self.img_loss_photo = mse_criterion(
    transformer_block(self.output_photo), transformer_block(self.input_photo))
self.img_loss = self.img_loss_photo

# Features loss.
self.feature_loss_photo = abs_criterion(self.output_photo_features, self.input_photo_features)
self.feature_loss = self.feature_loss_photo

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;运行时&quot;&gt;运行时&lt;/h2&gt;

&lt;p&gt;在unity中， 我们使用compute shader实现了一套跟TensorFlow中的相同的前向传播网络。 这里只是实现了Encoder和Decoder， 并没有实现Discrimator, 因为只有训练的时候用到了Discrimator.&lt;/p&gt;

&lt;p&gt;在网络层中 Batch-normal由于为了求整体的均值和方差， 需要遍历当前层每个深度的layer，需要使用归纳算法-reduce, 使计算效率时间复杂度由n变成logn, 所以设计网络的时候尽量减少了类似的操作，在&lt;a href=&quot;i4&quot;&gt;CompVis&lt;/a&gt;的设计网络中，decoder使用了九个残差模块， 为了性能我们这里减少到了一个。 参数规模也由大概48M减少到12M， 却实现了类似的效果。&lt;/p&gt;

&lt;p&gt;由于受限于compute shader的语法， 我们在定义thread group时候， thread的大小不能超过1024，thread-z不能超过64， group的组成是vec3的格式， 而且需要是32或者64的倍数， 因此我们在设计网络的时候，每一个的layer尽量去靠近这些特性， 致使GPU发挥出最大的性能。 （CS5 group thread个数最多是1024， 而CS4最多支持到512，Apple的平台最多支持到CS4，这里需要注意下）。&lt;/p&gt;

&lt;h2 id=&quot;数据导出&quot;&gt;数据导出&lt;/h2&gt;

&lt;p&gt;tensorflow 训练数据集有自己的序列化方式，大概是protobuf,  google也提供了一套api, 去获取里面的张量Tensor。&lt;/p&gt;

&lt;p&gt;通过训练集之后导出的checkpoint文件大小超过一个G，如果把这么庞大的参数文件导入到unity中所开销的内存空间是无法想象的。&lt;/p&gt;

&lt;p&gt;通过遍历checkpoint发现所有的tensor发现， 每一层layer， 网络中的每个参数都对应了两个Adam对象，所以我们写了一套工具，导出数据的时候过滤掉Adam对象，使其大小减小到之前的1/3.&lt;/p&gt;

&lt;p&gt;在上面提到，discriamtor只在训练的时候用到，而且其参数规模远超generator, 这里我们在导出的时候也需要对其过滤掉。&lt;/p&gt;

&lt;p&gt;通过上面的操作我们导出的参数规模大概是48M，由于可以去掉预算复杂且效率不高的残差网络模块，参数规模进一步缩小到十几兆。 这还是我们采用float存储的格式， 如果对精度要求不高， 采用half的数据格式 5M-6M之间， 我觉得这个大小都手机平台还是可以接受的。&lt;/p&gt;

</description>
        <pubDate>Thu, 02 May 2019 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2019/nnstyle/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2019/nnstyle/</guid>
        
        <category>Unity</category>
        
        <category>前端开发</category>
        
        <category>Tensorflow</category>
        
        <category>人工智能</category>
        
        <category>Python</category>
        
        <category>工具</category>
        
        
      </item>
    
      <item>
        <title>CG中修饰符in out inout的使用</title>
        <description>&lt;p&gt;关于cg中的修饰符关键字in out inout的区别, 网上的大致描述如下：&lt;/p&gt;

&lt;p&gt;参数传递是指：函数调用实参值初始化函数形参的过程。在 C\C++中，根据形参值的改变是否会导致实参值的改变，参数传递分为“值传递（pass-by-value） ” 和“引用传递（pass-by-reference） ”。按值传递时，函数不会访问当前调用的实参，函数体处理的是实参的拷贝，也就是形参，所以形参值的改变不会影响实参值；引用传递时，函数接收的是实参的存放地址，函数体中改变的是实参的值。&lt;/p&gt;

&lt;p&gt;C\C++ 采取指针机制构建引用传递，所以通常引用传递也称为“指针传递”。Cg 语言中参数传递方式同样分为“值传递”和“引用传递”，但指针机制并不被 GPU 硬件所支持，所以 Cg 语言采用不同的语法修辞符来区别“值传递”和“引用传递”。这些修辞符分别为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
in: 修辞一个形参只是用于输入，进入函数体时被初始化，且该形参值的改变不会影响实参值，这是典型的值传递方式。

out: 修辞一个形参只是用于输出的，进入函数体时并没有被初始化，这种类型的形参一般是一个函数的运行结果；

inout: 修辞一个形参既用于输入也用于输出，这是典型的引用传递。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;于是写了一段测试程序， 只在红色通道上输出uv的u值&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;#pragma vertex vert
#pragma fragment frag

struct appdata
{
	float4 vertex : POSITION;
	float2 uv : TEXCOORD0;
};

struct v2f
{
	float2 uv : TEXCOORD0;
	float4 vertex : SV_POSITION;
};

v2f vert (appdata v)
{
	v2f o;
	o.vertex = UnityObjectToClipPos(v.vertex);
	o.uv = v.uv;
	return o;
}

fixed4 frag (v2f i) : SV_Target
{
	return fixed4(i.uv.x,0,0,1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在unity里运行可以看到的效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/inout1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;现在代码作如下修改，&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;float test(in float x)
{
    x = clamp(x * 2,0,1);
    return x;
}

fixed4 frag (v2f i) : SV_Target
{
    i.uv.x = test(i.uv.x);
    return fixed4(i.uv.x,0,0,1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/inout2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;test() 函数去掉关键字修饰符in， 发现运行效果跟上图一致。&lt;/p&gt;

&lt;p&gt;这说明in 是pass with value。 参数是拷贝过去的， 在实际操作的过程中in可以不写。&lt;/p&gt;

&lt;p&gt;这时我们把test里的in改为out 发现编译器报错， error内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;variable &#39;x&#39; used without having been completely initialized
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;报错提示x没有初始化，于是我们再次修改为test函数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;void test(out float x)
{
    x=0.2;
    x = clamp(x * 2,0,1);
}

fixed4 frag (v2f i) : SV_Target
{
    test(i.uv.x);
    return fixed4(i.uv.x,0,0,1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;error消除，此时test需要给x一个初始化的值，而且test此时并没有返回值，运行的结果也传递给外部。运行效果如下，&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/inout3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接着测试inout，此时去掉test的x的初始值，作如下修改：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;void test(inout float x)
{
    x = clamp(x * 2,0,1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行结果如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/inout2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;说明inout就是引用，即pass with reference。&lt;/p&gt;

</description>
        <pubDate>Sun, 03 Mar 2019 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2019/inout/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2019/inout/</guid>
        
        <category>Unity</category>
        
        <category>前端开发</category>
        
        
      </item>
    
      <item>
        <title>PBR实现染色效果</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Unity5之后加入新的渲染方式 PBR。很多端游（洛奇英雄传），手游（楚留香）都有染色系统的存在。时装染色过程中支持自由调节色彩、饱和度以及明暗度，所以每一位少侠染色过后的衣服都是独一无二的，每一件外观都能改造出不同风格，走在街上再也不怕撞衫啦。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;网易《楚留香》手游实现的染色系统。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/pbr30.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了实现类似的效果，我们在手游在还原端游的同时，也加入了染色系统。下面是一个自己实现的染色例子，感兴趣的读者，可以前往下载地址，&lt;a href=&quot;https://github.com/huailiang/pbr_proj&quot;&gt;点击这里&lt;/a&gt;。Unity里打开Rendering/Art/Example_ROLE场景即可。实现的效果具体参考下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/dye.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;染色系统的实现不再基于对纹理简单的采样, 而是程序里自定义颜色。shader的属性里设置了R,G,B 三个通道的颜色，可以通过材质Inspector窗口自定义颜色。piexl shader中去混合这些颜色。在这个例子当中，我们只是定义了是三个通道，往往并不能满足策划们的需求。&lt;/p&gt;

&lt;p&gt;在实际情况中，我们通过uv划分，来支持更多的染色区域。 比如说uv.y 在[1,2]区间可以染色成一种颜色，在uv.y 在[2,3]区间还可以染成另外一种颜色，&lt;/p&gt;

&lt;p&gt;类似的原理来支持更多的颜色混合。在我们正在研发的手游中，定义了五个通道（RGB三个通道+2个uv区分）来实现混合的效果。由于游戏还在研发中，这里就不多赘诉了。&lt;/p&gt;

&lt;p&gt;至于颜色混合原码，这里贴出颜色混合的部位核心代码，至于完整的代码，可以去前面贴出的github地址前往下载：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hlsl&quot;&gt;
float3 diffuseColor1 = 
        (_ColorR.rgb * texColor.r * _ColorR.a +
         _ColorG.rgb * texColor.g * _ColorG.a + 
         _ColorB.rgb * texColor.b * _ColorB.a) * _Color.rgb * float(8);

float2 newuv= float2(i.uv0.x-1,i.uv0.y);
float4 newColor = tex2D(_MainTex,TRANSFORM_TEX(newuv, _MainTex));
float3 diffuseColor2 = (newColor.rgb * _Color.rgb);

float uvlow = step(i.uv0.x, 1); 
float uvhigh = 1 - uvlow;
float3 diffuseColor = diffuseColor1 * uvlow + diffuseColor2 * uvhigh;
float alpha = (_ColorR.a + _ColorG.a + _ColorB.a) * 0.7 + uvhigh * 0.3;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用这套染色系统，对mesh有一定的要求，需要诸如衣服颜色这些固定颜色的部位使用R,G,B中的一种颜色，里面只有灰度变化。对于像皮肤肉色这种变化的且追求细节的部位，纹理绑定的uv.x区间需要超出1，这部分区域我们不再混合颜色，而是直接对原纹理进行采样。&lt;/p&gt;

&lt;p&gt;读者感兴趣的话，可以通过工具QUVEditor uv工具查看。unity的QUVEditor可以在&lt;a href=&quot;http://www.qtoolsdevelop.com/&quot;&gt;这里下载&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/pbr31.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 12 Nov 2018 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2018/dye/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2018/dye/</guid>
        
        <category>Unity</category>
        
        <category>前端开发</category>
        
        
      </item>
    
      <item>
        <title>强化学习-游戏AI Trainning (四)</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;记得半年前，我介绍过强化学习的算法，比如说Q-learning, sara, DQN. 今天我们来介绍两种新的强化学习的算法Policy Gradient。  相较于之前的方法，今天的两种方式更加深入的使用深度神经网络。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;本文对应到github工程代码地址：&lt;a href=&quot;https://github.com/huailiang/bird&quot;&gt;https://github.com/huailiang/bird&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/huailiang/bird
#切换到PolicyGradient
git checkout PolicyGradient
#切换到ppo分支
git checkout ppo
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;introduce&quot;&gt;Introduce&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;Policy Gradient&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Policy gradient 是 RL 中另外一个大家族, 他不像 Value-based 方法 (Q learning, Sarsa), 但他也要接受环境信息 (observation), 不同的是他要输出不是 action 的 value, 而是具体的那一个 action, 这样 policy gradient 就跳过了 value 这个阶段. 而且个人认为 Policy gradient 最大的一个优势是: 输出的这个 action 可以是一个连续的值, 之前我们说到的 value-based 方法输出的都是不连续的值, 然后再选择值最大的 action. 而 policy gradient 可以在一个连续分布上选取 action.&lt;/p&gt;

&lt;p&gt;policy gradient 是一种基于 整条回合数据 的更新, 也叫 REINFORCE 方法. 这种方法是 policy gradient 的最基本方法, 有了这个的基础, 我们再来做更高级的。更新网络中的参数，具体的实现方式如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-reinforcement/re21.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;损失函数即：&lt;b&gt;loss= -log(prob)*vt&lt;/b&gt;&lt;br /&gt;
今天我们的目的不是推导此公式是怎么得来的，而是利用此公式，来实现机器学习的目的。若你想详细了解算法详细的推导过程，可以参考&lt;a href=&quot;https://blog.csdn.net/qq_30615903/article/details/80747380&quot;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;通过我们公式可以看到，当奖励（vt）越大的时候，在loss不断减少的情况下，prob发生的几率会越大。反之，奖励（vt)越小的情况，随着梯度，prob发生的机会也越小。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;PPO&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PPO 是 OpenAI 发表的 Trust Region Policy Optimization,基于 Actor-Critic 算法。根据 OpenAI 的官方博客, PPO 已经成为他们在强化学习上的默认算法。&lt;/p&gt;

&lt;p&gt;如果一句话概括 PPO: OpenAI 提出的一种解决 Policy Gradient 不好确定 Learning rate (或者 Step size) 的问题. 因为如果 step size 过大, 学出来的 Policy 会一直乱动, 不会收敛, 但如果 Step Size 太小, 对于完成训练, 我们会等到绝望. PPO 利用 New Policy 和 Old Policy 的比例, 限制了 New Policy 的更新幅度, 让 Policy Gradient 对稍微大点的 Step size 不那么敏感.&lt;/p&gt;

&lt;video id=&quot;video&quot; controls=&quot;&quot; preload=&quot;none&quot; poster=&quot;/img/in-post/post-reinforcement/re6.jpg&quot; width=&quot;674&quot; height=&quot;379&quot;&gt;
      &lt;source id=&quot;mp4&quot; src=&quot;https://morvanzhou.github.io/static/results/reinforcement-learning/6-4-demo_openai.mp4&quot; type=&quot;video/mp4&quot; /&gt;
      &lt;p&gt;Your user agent does not support the HTML5 Video element.&lt;/p&gt;
&lt;/video&gt;

&lt;p&gt;官方的paper对ppo有两种实现方式。分别是 KL penalty和Clip的方式。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-reinforcement/re22.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PPO 是一套 Actor-Critic 结构, Actor 想最大化 J_PPO, Critic 想最小化 L_BL. Critic 的 loss 好说, 就是减小 TD error. 而 Actor 的就是在 old Policy 上根据 Advantage (TD error) 修改 new Policy, advantage 大的时候, 修改幅度大, 让 new Policy 更可能发生. 而且他们附加了一个 KL Penalty (惩罚项, 不懂的同学搜一下 KL divergence), 简单来说, 如果 new Policy 和 old Policy 差太多, 那 KL divergence 也越大, 我们不希望 new Policy 比 old Policy 差太多, 如果会差太多, 就相当于用了一个大的 Learning rate, 这样是不好的, 难收敛.&lt;/p&gt;

&lt;h2 id=&quot;purpose&quot;&gt;purpose&lt;/h2&gt;

&lt;p&gt;此次我们的目的，还是通过机器学习，使游戏中的小鸟宝宝学会飞翔。我们依旧采用unity来表现，而使用python来拟合神经网络，训练数据。二者之间通过socket来实现数据通信。&lt;br /&gt;
最终实现的效果如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-reinforcement/re10.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;PolicyGradient&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据github提供的项目，切到PolicyGradient分支，在根目录找到PolicyGradients文件夹,所有的实现都在里面了。&lt;/p&gt;

&lt;p&gt;我们使用两个全连接层（fc1,fc2）设计我们的神经网络，&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;with tf.name_scope(&#39;inputs&#39;):
    self.tf_obs = tf.placeholder(tf.float32, [None, self.n_features], name=&quot;observations&quot;)
    self.tf_acts = tf.placeholder(tf.int32, [None, ], name=&quot;actions_num&quot;)
    self.tf_vt = tf.placeholder(tf.float32, [None, ], name=&quot;actions_value&quot;)
    # fc1
    layer = tf.layers.dense(
        inputs=self.tf_obs,
        units=10,
        activation=tf.nn.tanh,  # tanh activation
        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),
        bias_initializer=tf.constant_initializer(0.1),
        name=&#39;fc1&#39;
    )
    # fc2
    all_act = tf.layers.dense(
        inputs=layer,
        units=self.n_actions,
        activation=None,
        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),
        bias_initializer=tf.constant_initializer(0.1),
        name=&#39;fc2&#39;
    )

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们根据之前说的policygradient的算法，来设计损失函数（Loss Function）。&lt;/p&gt;

&lt;p&gt;即：loss= -log(prob)*vt&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt; with tf.name_scope(&#39;loss&#39;):
    neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)
    loss = tf.reduce_mean(neg_log_prob * self.tf_vt)  # reward guided loss

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;训练的过程就是减少loss，即沿着梯度下降的方向跟新网咯中的参数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;with tf.name_scope(&#39;train&#39;):
    self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在tensorboard我们可以清楚看到整个网络的结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-reinforcement/re23.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;PPO&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据github提供的项目，切到ppo分支，在根目录找到ppo文件夹,所有的实现都在里面了。这里采用的是官方paper介绍的第二种算法（clip）。看到网络设计上ppo的实现方式很多，有基于连续的，有基于离散的，还有采用多线程来更新网络的参数。&lt;/p&gt;

&lt;p&gt;鉴于我们的设计目的是使小鸟学会飞翔，这里我们的实现方式采用的是离散的方式实现ppo。因为我们小鸟采取的动作就两种（fly or pad）。&lt;/p&gt;

&lt;p&gt;根据OpenAI官方的Paper，我们设计我们的网络结构。 由于PPO是基于A3C，又是利用两个网络的接近程度来更新网络中的参数，所以这里至少有三个神经网络，即critic, actor1(pi), actor2(oldpi)。&lt;/p&gt;

&lt;p&gt;我们实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#critic
with tf.variable_scope(&#39;critic&#39;):
    l1 = tf.layers.dense(self.tfs, 100, tf.nn.relu)
    self.v = tf.layers.dense(l1, 1)
    self.tfdc_r = tf.placeholder(tf.float32, [None, 1], &#39;discounted_r&#39;)
    self.advantage = self.tfdc_r - self.v
    self.closs = tf.reduce_mean(tf.square(self.advantage))
    self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)

# actor  pi &amp;amp;oldpi
    self.pi, pi_params = self._build_anet(&#39;pi&#39;, trainable=True)
    oldpi, oldpi_params = self._build_anet(&#39;oldpi&#39;, trainable=False)

# actor
def _build_anet(self, name, trainable):
    with tf.variable_scope(name):
        l_a = tf.layers.dense(self.tfs, 200, tf.nn.relu, trainable=trainable)
        a_prob = tf.layers.dense(l_a, A_DIM, tf.nn.softmax, trainable=trainable)
    params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)
    return a_prob, params

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们根据pi和old pi之间测差异决定Loss：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;a_indices = tf.stack([tf.range(tf.shape(self.tfa)[0], dtype=tf.int32), self.tfa], axis=1)
pi_prob = tf.gather_nd(params=self.pi, indices=a_indices)   # shape=(None, )
oldpi_prob = tf.gather_nd(params=oldpi, indices=a_indices)  # shape=(None, )
ratio = pi_prob/oldpi_prob
surr = ratio * self.tfadv                       # surrogate loss

with tf.variable_scope(&#39;loss&#39;):
    self.aloss = -tf.reduce_mean(tf.minimum(        # clipped surrogate objective
    surr,
    tf.clip_by_value(ratio, 1. - EPSILON, 1. + EPSILON) * self.tfadv))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在tensorboard观察网络结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-reinforcement/re25.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 10 Nov 2018 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2018/ppo/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2018/ppo/</guid>
        
        <category>Unity</category>
        
        <category>前端开发</category>
        
        <category>人工智能</category>
        
        <category>强化学习</category>
        
        
      </item>
    
      <item>
        <title>为你的游戏定制lua</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;由于lua解释执行，且是一款轻量级的脚本语言，所以作为脚本无数次被广大游戏项目采用。市面上lua插件多如牛毛，像nlua,ulua,slua,xlua,tolua。 每一款插件或多或少有这样或那样的优点缺点。 我觉得王道就是自己写一个lua插件，定制给游戏，游戏需要什么功能，就加入什么功能，移除不必要的模块，加入自己需要的模块。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;本文对应到github工程代码地址：&lt;a href=&quot;https://github.com/huailiang/ulua_proj&quot;&gt;https://github.com/huailiang/ulua_proj&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;为什么要定制&quot;&gt;为什么要定制&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;移除不必要的模块，比如说luasocket，qllite这些，加入些新的特性，比如说lua protobuf3.x&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主流的lua插件都是针对c#的代码修复，越来越多的游戏逻辑移植到c++里，定制就意味着可以和引擎逻辑编入同一个库中，自然可以用来修复c++里的模块&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;为什么选择lua53&quot;&gt;为什么选择lua53&lt;/h2&gt;

&lt;p&gt;为什么选择lua53, 而不是效率更高的luajit。 主要是考虑到项目使用lua来修复既有的逻辑模块，而不是使用lua来开发新的模块。（不是绝对的使用lua不开发新功能，比如说活动系统这种频繁更新的模块可以使用lua来开发，只是这种需求比较少，而且对性能要求不高）&lt;/p&gt;

&lt;p&gt;还有一个原因是使用lua53，是因为lua53天然支持int64，游戏逻辑使用c#,c++这种高阶语言编写的，很容易用到int64这种数据格式。&lt;/p&gt;

&lt;h2 id=&quot;编译lua源码踩的坑&quot;&gt;编译lua源码踩的坑&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ios build error: ‘system’ is unavailable: not available on iOS&lt;br /&gt;
iOS11废除了system之后,rug如果使用xcode9以上的版本编译都会报此错误，解决方法就是：&lt;br /&gt;
将loslib.c中&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;int stat = system(cmd);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;改为:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;int stat = nftw(cmd, unlink_cb, 64, FTW_DEPTH | FTW_PHYS);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;引入头文件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;ftw.h&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;添加方法:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;int unlink_cb(const char *fpath, const struct stat *sb, int typeflag, struct FTW     *ftwbuf)
{
    int rv = remove(fpath);
    
    if (rv)
        perror(fpath);
    
    return rv;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ftw.h当然这是ios平台才有的头文件，所以你使用同一份代码编译编译Android的时候又会发现此文件找不到，所以可以使用#ifdef IOS 这样的宏区分开来。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Android build error: struct lconv has no…‘decimal_point’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;查询msdn，后知道：char *decimal_point, wchar_t *_W_decimal_point, Decimal-point character for nonmonetary quantities.对于非货币数量的小数点字符。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-lua/lua5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;于是将错误的地址进行如下修改：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#ifdef WIN32

       lcc = localeconv();   /* set structure containing local decimal point symbol */
       decimalpt = *(lcc-&amp;gt;decimal_point);
#else

       decimalpt = &#39;.&#39;;
#endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-lua/lua4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当然还有很多android还有类似log2的build error错误，后来我们决定采用类似xlua的套路：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;把差异化的东西编译到link文件，然后cmakelist这样配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;configure_file ( ${LUA_SRC_PATH}/luaconf.h.in ${CMAKE_CURRENT_BINARY_DIR}/luaconf.h )
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;protobuf&quot;&gt;Protobuf&lt;/h2&gt;

&lt;p&gt;最让我抓狂的是google官方居然不支持Lua版的protobuf。ulua,tolua使用的都是云风的gen_lua_protoc这款插件。但是gen_lua_protoc虽然支持protobuf, 但是只支持到protobuf 2.x版本。而且对应的lua版本也是lua5.1。&lt;/p&gt;

&lt;p&gt;xlua官方的github 工程中并没有支持到protobuf，  不过xlua的作者在自己的blog 集成了第三方protobuf3.x, , 然并没有发布到腾讯官方的公布的工程中。 xlua作者提供的xlua扩展工程叫build_xlua_with_libs，如果读者感兴趣，可以点击这里，&lt;a href=&quot;https://github.com/chexiongsheng/build_xlua_with_libs&quot;&gt;查看源码&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-lua/xLua.png&quot; alt=&quot;i2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;lua虽然支持了protobuf, 而且同时支持了2.x和3.x两个版本。但是依旧解决不了我项目的问题。 首先socket的收发协议都在c#。 c++ 和lua 模块中如果需要处理网络消息，都需要bytes来中转。xlua 就没有像ulua,tolua那样提供类似的功能。（其实ulua和tolua的处理归根结底还是gen_lua_protoc处理的）。&lt;/p&gt;

&lt;p&gt;于是我使用ulua的方式导出LuaStringBuf的方式，把LuaStringBuf传递给lua,lua接受到参数后，作为data使用pb.decode去反序列化，居然成功了。&lt;/p&gt;

&lt;p&gt;类似的我想把lua中序列化的对象传递给c#, c#却不认了。c#识别的是string对象。于是我不得转变思路。通过查看&lt;a href=&quot;https://github.com/starwing/lua-protobuf&quot;&gt;lua-protobuf源码&lt;/a&gt;，我了解到lua在序列化的时候(pb.encode),是向堆栈上lua_pushlstring，这个字符串是不随’\0’ 来结束的，说白了就是一串指定长度的unsigned char,对应到c#的byte[]。 我使用的方式的就是把string每个字符转成byte存到table中，然后传递到c#中对应的类型是LuaTable。 c#拿到之后再转换为byte[]。 最后使用此byte[]在c#反序列化。从&lt;a href=&quot;https://github.com/huailiang/ulua_proj&quot;&gt;例子&lt;/a&gt;里可以看到反序列化的结果。&lt;/p&gt;

&lt;h2 id=&quot;bytecode&quot;&gt;ByteCode&lt;/h2&gt;

&lt;p&gt;Lua 导出bytecode&lt;/p&gt;

&lt;p&gt;lua使用bytecode 的好处主要有两点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; a. 二进制文件，为了加密
 b. 编译后的中间件，提升效率
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;那么如何导出bytecode呢？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;luajit&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;a. 进入luajit\LuaJIT-2.0.1\src 目录

b. uajit -b 需要编译的lua文件路径 编译输出文件保存路径

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# luajit -b d:\src.lua d:\des.lua
luajit -b d:\src.lua d:\des.lua
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;luac (mac下)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;运行下面命令&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;curl -R -O http://www.lua.org/ftp/lua-5.3.1.tar.gz 
tar zxf lua-5.3.1.tar.gz 
cd lua-5.3.1 
make macosx test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装, 输入以下命令，会要求输入Password: 输入相应密码（你的密码），然后回车就自动安装了&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;配置编译器 sublime下执行Tools-&amp;gt;Build System-&amp;gt;New Build System 输入：&lt;/p&gt;

&lt;p&gt;{ &lt;br /&gt;
“cmd”: [“/usr/local/bin/lua”, “$file”], &lt;br /&gt;
“file_regex”: “^(…?):([0-9]):?([0-9]*)”, &lt;br /&gt;
“selector”: “source.lua”&lt;br /&gt;
} &lt;br /&gt;
保存为Lua.sublime-build，然后Tools-Build System上就能选择lua来编译脚本了&lt;/p&gt;

&lt;p&gt;luac生成bytecode, 使用如下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;luac -o test.luac test.lua
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：&lt;/p&gt;

&lt;p&gt;如果项目在PC下正常运行，但是安装到Android手机就报错：ulua.lua: cannot load incompatible bytecode，那么说明你的运行时luajit和编译时luajit版本不一致，你需要删除LuaEncoder文件夹下的luajit，然后，把LuaFramework下的luajit拷贝过来，然后在运行就可以了。&lt;/p&gt;

&lt;p&gt;如果运行时候出现这个报错&lt;/p&gt;

&lt;p&gt;LuaException: error loading module Main from CustomLoader,&lt;br /&gt;
Main: size_t size mismatch in precompiled chunk&lt;br /&gt;
解决： 所使用的luac编译工具得区分32、64位 , 安卓需在32位的编译文件&lt;/p&gt;

&lt;p&gt;https://github.com/Tencent/xLua/issues/356&lt;/p&gt;

&lt;p&gt;https://www.jianshu.com/p/3c49cf454502&lt;/p&gt;

&lt;h2 id=&quot;结语&quot;&gt;结语&lt;/h2&gt;

&lt;p&gt;好久没有更新博客了，最近的项目工作量上来了，之后相信会越来越忙了，祝大家工作顺利。&lt;/p&gt;

</description>
        <pubDate>Tue, 02 Oct 2018 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2018/ulua/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2018/ulua/</guid>
        
        <category>Unity</category>
        
        <category>前端开发</category>
        
        <category>Lua</category>
        
        <category>工具</category>
        
        
      </item>
    
      <item>
        <title>PBR基于物理的着色</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Unity5之后加入新的渲染方式 PBR。PBR是一种着色和渲染技术，用于更精确的描述光如何与物体表面互动。PBS（Physically Based Shading）在有一些地方也叫PBR（Physically Based Rendering），是一个基于物体表面材质属性的着色方法。与之前的Blinn-Phong等算法不同。PBS通过对物体表面的材质属性与周围光照信息来进行着色计算。PBS着色系统中，一个物体不仅受到光源的影响，还会受到周围环境的影响。 这会使得整个场景更加真实。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PBS有一个大前提，就是它能够满足光能传播过程中的能量守衡。能量守衡体现在三个方面。&lt;/p&gt;

&lt;p&gt;1、一个对象反射出来的光照信息，不可能超过它接受到的信息。也就是说，全反射是一个物体的极限。&lt;/p&gt;

&lt;p&gt;2、一个物体越光亮，那么它的颜色信息应该越少。（可以看出，refection 和 diffuse 应该是一个插值关系）&lt;/p&gt;

&lt;p&gt;3、一个物体越平滑，那么它的高亮点会越小，越亮。&lt;/p&gt;

&lt;p&gt;下面以一个自己实现的 pbr 例子，例子下载地址，&lt;a href=&quot;https://github.com/huailiang/pbr_proj&quot;&gt;点击这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;关于 pbr shader的实现这里就不再赘述了，工程附带的 readme 推导公式、源码都给的很详细。本文主要是通过此例子来验证 pbr 的一些特性。&lt;/p&gt;

&lt;p&gt;选中材质，开启 OpenDebug 选项。通过输出参数控制来调试 pbr 的各种效果。当然这些只是在编辑器里预览，游戏运行时，可以通过 material.DisableKeyword()来关闭此选项，来避免不必要的计算。&lt;/p&gt;

&lt;p&gt;我们都知道PBR 材质金属性越强，反射的光越少，因为大部分光都被金属吸收了，转化为热能或者说电能。&lt;br /&gt;
为了验证 pbr 这一特性，我们 debugmode 选中 Diffuse，如下图所示，拖拽 Matillic 属性来改变材质的金属性发现：matallic 值越大，材质的颜色越暗；反之，材质的颜色越亮。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/pbr1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面我们再验证 PBR 材质的能量守恒性质：&lt;/p&gt;

&lt;p&gt;我们把 Debug Mode 选中 None, 不要开启边缘发光效果，我们拖拽 Gloss 选项来改变材质的光滑度。通过滑动，我们可以发现，Gloss 越小，漫反射（高光部分）区域越大，但光线锐度越小；Gloss 越大，虽然高光区域越小，但光线的亮度越高，锐度越犀利。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/pbr4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多的调试选项这里就不一一列举了，比如查看材质发现的方向，PBR 公式计算的过程中 法线分布函数、微平面遮挡系数、Fresnel 现象等等，等等都可以通过 DebugMode 来调试。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/pbr6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;然而，尽管实现了 pbr 的特性，往往却不能满足美术或者说策划大大们的需求，这些需求往往并不是更真实的着色，比如说我们在展示 avatar 的时候，需要一圈外发光效果。有时候外发光的颜色使我们场景里主光的颜色，有时外发光的颜色是某一个指定的颜色。&lt;/p&gt;

&lt;p&gt;在材质的选项中，我们没有勾选 SpecialRimColor,外发光的颜色为主光的颜色，当我们勾选之后，可以指定一个特定的外发光颜色。效果如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/pbr5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;处理半透明有多重方式，主要是 AlphaTest 和 AlphaBlend 两种。在例子的Example_ALPHA的 scene 中，我们给了四种 alpha 混合方式：Opaque、 Cutout、 CutoutTansparent、 Transparent四种裁剪方式。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-pbr/pbr2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Opaque 的渲染队列是Geometry， RenderType 是Opaque 效果是右下角，没有透明度也没有Alpha裁剪&lt;/p&gt;

&lt;p&gt;Cutout 的渲染队列是AlphaTest，RenderType 是Opaque 效果如左下角，没有透明度但有 Alpha 裁剪&lt;/p&gt;

&lt;p&gt;CutoutTransparent 渲染队列是Transparent， RenderType 是TransparentCutout， 有透明度也有 Alha 裁剪&lt;/p&gt;

&lt;p&gt;Transparent 的渲染队列是Transparent， RenderType 是Transparent， 有透明度 Alpha混合处理&lt;/p&gt;

&lt;p&gt;结语：我们可以使用 PBR，当然也可以再之基础上添加更多的效果，艺术的大脑是无限的。&lt;/p&gt;

</description>
        <pubDate>Sat, 04 Aug 2018 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2018/pbr/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2018/pbr/</guid>
        
        <category>Unity</category>
        
        <category>前端开发</category>
        
        
      </item>
    
      <item>
        <title>Unity 大地形加载研究</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;随着吃鸡手游的火爆，大世界、大地形类的手游正在变得越来越火热。想要大世界观的直面感受，大地形的研究课题也提上开发日程。我们都知道unreal 引擎直接提供了Level Streaming Volume方式的大地形加载方式，无需要过多的代码，就可以动态的生成大场景了。本文主要介绍unity 引擎下自己实现的大地形加载方式。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Unity 大地形研究&lt;/p&gt;

&lt;p&gt;*github 对应的地址，点击&lt;a href=&quot;https://github.com/huailiang/terrain_proj&quot;&gt;这里&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;针对美术的大世界场景进行分块切割&lt;/li&gt;
  &lt;li&gt;动态策略加载分块地形&lt;/li&gt;
  &lt;li&gt;地形动态生成assetbundle&lt;/li&gt;
  &lt;li&gt;分块之后，lightmap的索引和偏移重新计算&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;切割大地形&quot;&gt;切割大地形&lt;/h3&gt;

&lt;p&gt;打开 unity, 在菜单栏点击 Terrain-&amp;gt;Slicing 即可以切割大地形，代码会自动遍历 Hirerachy 里的地形，切割4X4的16块，切割好的地形默认会存在 Resources 目录下，生成一个地形 gameobject 同名的文件夹。&lt;/p&gt;

&lt;p&gt;除了地形分片资源，这里还会生成地形和物件相关的数据信息，保存成二进制文件，保存在同一目录下。&lt;/p&gt;

&lt;p&gt;二进制记录的内容代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c#&quot;&gt;FileStream fs = new FileStream(path, FileMode.OpenOrCreate, FileAccess.Write);
BinaryWriter writer = new BinaryWriter(fs);

//这里我分割的宽和长度是一样的.这里求出循环次数,TerrainLoad.SIZE要生成的地形宽度,长度相同
//高度地图的分辨率只能是2的N次幂加1,所以SLICING_SIZE必须为2的N次幂
int size = (int)terrain.terrainData.size.x / trnconst.SLICE;
Vector3 pos = terrain.transform.position;
writer.Write(pos.x);
writer.Write(pos.y);
writer.Write(pos.z);
writer.Write(size);
writer.Write(terrain.treeDistance);
writer.Write(terrain.treeBillboardDistance);
writer.Write(terrain.treeCrossFadeLength);
writer.Write(terrain.treeMaximumFullLODCount);
writer.Write(terrain.detailObjectDistance);
writer.Write(terrain.detailObjectDensity);
writer.Write(terrain.heightmapPixelError);
writer.Write(terrain.heightmapMaximumLOD);
writer.Write(terrain.basemapDistance);
writer.Write(terrain.lightmapIndex);
writer.Write(terrain.castShadows);
WriteParts(writer);
writer.Flush();
writer.Close();
fs.Close();
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;分段加载地形和物件&quot;&gt;分段加载地形和物件。&lt;/h3&gt;

&lt;p&gt;点击 Terrain-&amp;gt;Load 会加载分片地形， 并且根据地形分片生成一个对应的 collider.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post_terrain/2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;类似图片的展示的一样，走进走出collider 的 triger 就会触发地形的加载和卸载，实现过程类似于 Unreal引擎实现的Level Streaming Volume。为了避免玩家在collider边界频繁的走动从而出发频繁的内存的加载和卸载，可以在卸载加载的时候加一定的延时。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-csharp&quot;&gt;
private void OnTriggerEnter(Collider other)
{
    TerrainLoadMgr.sington.LoadItem(x, y);
}


private void OnTriggerExit(Collider other)
{
    TerrainLoadMgr.sington.UnloadItem(x, y);
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;部件的加载&quot;&gt;部件的加载&lt;/h3&gt;

&lt;p&gt;部件如场景里的石头这个物件，他的加载跟着地形分片一同加载、卸载。而不是以 player 为中心做四叉树来管理场景的加载卸载。&lt;/p&gt;

&lt;p&gt;项目中引用了第三人称控制器，可以使用W、A、D、S快捷键在场景中走动看看地形动态加载的效果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post_terrain/3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;lightmap生成assetbundle&quot;&gt;lightmap生成assetbundle&lt;/h3&gt;

&lt;p&gt;点击 Terrain-&amp;gt;生成lightmap资源，即可以把当前场景的lightmap 贴图全部达成assetbundle. 在打包lightmap贴图的同时，会生成一个二进制文件被打到同一个assetbundle中，这个二进制文件记录了当前场景里所有render的lightmap的index索引和offsetscale偏移。&lt;/p&gt;

&lt;p&gt;我们使用AssetStudio 来查看assetbundle 里的内容，可以清楚看到资源的分布：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post_terrain/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;注意&quot;&gt;注意：&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;地形切割之后，隐藏之前的原有地形，放掉TerrainLoadEditor.Load函数里的注释，把所有的地形分片加载到场景&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;进行场景烘焙，这样lightmap记录的所有切割地形的索引和偏移。lightmap生成之后，把对应的贴图打包到对应的assetbundle。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;运行时，删去所有场景里分片地形，所有的地形都是动态加载的。assetbundle先找到里面的bytes数据，根据数据生成lightmapdata赋值给LightmapSetting,再动态算场景里物件的render所对应的lightmap偏移和索引。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;设置render的lightmap索引和偏移，会打断unity自身的static batch，为了减轻gpu的渲染负担，在所有设置好偏移和索引之后，可以使用CombineInstance 和MaterialPropertyBlock 等技术进行合批和优化。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;github工程里有三个scene：&lt;/p&gt;

&lt;p&gt;race_track_lake： 用来测试地形的切割和加载&lt;/p&gt;

&lt;p&gt;race_track_lake2：测试lightmap的动态加载(ab)和偏移 不考虑地形切割&lt;/p&gt;

&lt;p&gt;race_track_lake3：地形切割且使用lightmap的动态加载&lt;/p&gt;

</description>
        <pubDate>Sun, 15 Jul 2018 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2018/terrain/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2018/terrain/</guid>
        
        <category>Unity</category>
        
        <category>工具</category>
        
        <category>前端开发</category>
        
        
      </item>
    
      <item>
        <title>Unity 植入 c++版protobuf</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;现在使用Unity引擎开发游戏使用的脚本语言越来越灵活了。就拿《王者荣耀》来说吧，战斗的核心都采取 c++的语言来实现，外围系统都是采用 csharp的开发的。使用 c++指针操作不断有更高的执行效率，作为 native 语言也不会有 mono 内存的开销。使用 csharp 语言语法简单，对面向对象的支持也比较好，适合快速开发和版本迭代。还有使用 lua 脚本来做游戏的热更新。这样问题来了，解析数据就需要三个版本的 protobuf 集成在游戏里了。这个暂且只讨论unity里集成c++版本 protobuf 的流程。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先去 google 的 github 网站下载稳定版本的 protobuf。读者需要的话，点击&lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;这里&lt;/a&gt;。这里我们选择3.5.1版本，完整的 protobuf 包含了大量的测试的代码，这里我们只需要一个完整的 lib 项目就可以了，没有用的测试代码如果不想编进库里，可以删去测试代码部分。&lt;/p&gt;

&lt;p&gt;先看下，c#与 c++交互的部分。我们传给 c++ protobuf 序列化的二进制数组，在 c++里解析二进制。所以传递参数是 char* 指针和 bytes数组对应的长度，csharp 里我们把二进制数组转换成可以 c++交互的Intptr 指针。定义如下：&lt;/p&gt;

&lt;p&gt;csharp 与 c++交互接口定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c#&quot;&gt;#if UNITY_IPHONE || UNITY_XBOX360
	[DllImport(&quot;__Internal&quot;)]
#else
    [DllImport(&quot;ptotobuf-lib&quot;)]
#endif
    public static extern int iDeserial(IntPtr pb, int length);

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;csharp 获取 bytes 数组的首地址可以通过GCHandle来获取：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c#&quot;&gt; GCHandle hObject = GCHandle.Alloc(bytes, GCHandleType.Pinned);
IntPtr pObject = hObject.AddrOfPinnedObject();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;c++部分与 c#交互的部分需要写在 extern里, 参数传递指针和 bytes 数组的长度：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;extern &quot;C&quot;
{

	int iDeserial(const char* pb, int length)
	{
		XNet::Student student;
		if (!student.ParseFromArray(pb, length))
		{
			printf(&quot;parse student error&quot;);
		}
		else
		{
			int age = student.age();
			int num = student.num();
			std::cout &amp;lt;&amp;lt; &quot;age: &quot; &amp;lt;&amp;lt; age &amp;lt;&amp;lt; std::endl;
			std::cout &amp;lt;&amp;lt; &quot;num:&quot; &amp;lt;&amp;lt; num &amp;lt;&amp;lt; std::endl;
			return age + num;
		}
		return 0;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面就是不同平台部分的编译 dll 部分&lt;/p&gt;

&lt;p&gt;打进 dll里的内容包含两部分：1.google protobuf 的c++代码 2.与 c#交互的代码和游戏逻辑代码&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Windows 部分&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首先编出来的 dll 得指定到 x64平台， c/c++附加包含目录指定到 google文件夹所在你的目录。SDL 检查选择否。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cpp/1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在 vs 工程点击生成即可以生成 64位 的 dll，把编译好的 dll copy 到unity plugins 目录里，就可以调试看到结果了。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Android 部分&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;手机平台比较复杂，特别是 Android 机型众多，区分 arm 和 x86。而 arm 还有各种细分的架构，不过通过解压 unity 生成的 apk 可以发现，libs的目录只有 armv7和 x86两种平台，其他高阶的构架兼容低阶架构，就不需要我们再去编了。&lt;/p&gt;

&lt;p&gt;编译 android 的 c++代码，我们使用 android 原生支持的 NDK，我们只需要配置好Android.mk和Application.mk两个文件就好了。&lt;/p&gt;

&lt;p&gt;所以在编译 so 之前，需要里下载好 ndk，google官方的下载地址点击&lt;a href=&quot;https://developer.android.google.cn/ndk/downloads/&quot;&gt;这里&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;在Application.mk 中声明编译的平台，使用的 Android 版本：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;APP_ABI          := armeabi-v7a x86
APP_OPTIM         := release
APP_PLATFORM      := android-14
#APP_BUILD_SCRIPT := Android.mk
APP_STL       := c++_static
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在Android.mk中配置所要编译的 c++文件，编译使用的宏。使用方式跟 linux 使用 gcc编译c++很类似。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
LOCAL_PATH := $(call my-dir)  

include $(CLEAR_VARS)

LOCAL_CFLAGS:= -DHAVE_PTHREAD=1

#  c++目录的相对路径
MY_FILES_PATH  :=  $(LOCAL_PATH)/

# c++后缀
MY_FILES_SUFFIX := %.cpp %.cc %.c

# 递归遍历目录下的所有的文件
rwildcard=$(wildcard $1$2) $(foreach d,$(wildcard $1*),$(call rwildcard,$d/,$2))

# 获取相应的源文件
MY_ALL_FILES := $(foreach src_path,$(MY_FILES_PATH), $(call rwildcard,$(src_path),*.*) ) 
MY_ALL_FILES := $(MY_ALL_FILES:$(MY_CPP_PATH)/./%=$(MY_CPP_PATH)%)
MY_SRC_LIST  := $(filter $(MY_FILES_SUFFIX),$(MY_ALL_FILES)) 
MY_SRC_LIST  := $(MY_SRC_LIST:$(LOCAL_PATH)/%=%)

# 去除字串的重复单词
define uniq =
  $(eval seen :=)
  $(foreach _,$1,$(if $(filter $_,${seen}),,$(eval seen += $_)))
  ${seen}
endef

# 递归遍历获取所有目录
MY_ALL_DIRS := $(dir $(foreach src_path,$(MY_FILES_PATH), $(call rwildcard,$(src_path),*/) ) )
MY_ALL_DIRS := $(call uniq,$(MY_ALL_DIRS))

# 赋值给NDK编译系统
LOCAL_SRC_FILES  := $(MY_SRC_LIST)
LOCAL_C_INCLUDES:= $(LOCAL_PATH)/

LOCAL_SHARED_LIBRARIES:= 
LOCAL_MODULE:= libprotobuf-lib
LOCAL_MODULE_TAGS := optional
LOCAL_LDLIBS += -llog
include $(BUILD_SHARED_LIBRARY)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下载好 ndk，把路径配置到环境变量中，就可以使用了 ndk-build 来编译 so 了。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;osx 部分&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;osx 需要把 c++代码编译成 bundle格式的库文件，需要新建一个 xcode 工程，选项如下图如下所示：&lt;br /&gt;
&lt;img src=&quot;/img/in-post/post-cpp/5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们把 c++引用到 xcode 工程， 然后设置好需要的宏：&lt;br /&gt;
&lt;img src=&quot;/img/in-post/post-cpp/2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;设置文件搜索路径：&lt;br /&gt;
&lt;img src=&quot;/img/in-post/post-cpp/3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;点击 build，bundle 文件就可以生成了。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ios 部分&lt;br /&gt;
osx 需要把 c++代码编译成 bundle格式的库文件，需要新建一个 xcode 工程，选项如下图如下所示：&lt;br /&gt;
&lt;img src=&quot;/img/in-post/post-cpp/6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ios 设置部分基本上和 osx 是一样的，记得一处地方要设置好，build 的对象要选择真机而不是模拟器，否则在真机上无法运行：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cpp/4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当所有平台的 c++库都编译完成之后，对应到 unity 的目录如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cpp/7.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可能介绍的语言比较简洁，不过对应的完整的工程我都上传到 github 去了，大家可以去下载下来，跑一遍流程之后，就一切都清楚了。&lt;/p&gt;

&lt;p&gt;**github 对应的地址，点击&lt;a href=&quot;https://github.com/huailiang/game_pb&quot;&gt;这里&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sun, 24 Jun 2018 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2018/protobuf/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2018/protobuf/</guid>
        
        <category>Unity</category>
        
        <category>C++</category>
        
        <category>前端开发</category>
        
        
      </item>
    
      <item>
        <title>Batch 合并Drawcall绘制HUD</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;hud是游戏中大量使用的元素，一个血条往往只需要很少的表现。在Unity中常用的UI大都是nGUI或者uGUI,因为图文层级交错或者Depth重叠，往往带来不少的Drawcall开销，从而导致游戏性能的开销。这里我们建议使用3D mesh的方式去绘制HUD，从而去优化性能。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;本文对应的源码已经公开到github,点击&lt;a href=&quot;https://github.com/huailiang/hud&quot;&gt;地址&lt;/a&gt;可以查阅。&lt;/p&gt;

&lt;h2 id=&quot;绘制血条&quot;&gt;绘制血条&lt;/h2&gt;

&lt;p&gt;我们使用c#代码-Hud.cs动态创建一个mesh。首先动态创建一个mesh，我们画四个三角一共使用八个顶点。设置好他们对于的本地坐标。创建好的mesh如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-hud/hud-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们将前面两个三角使用顶点色-红色来表示血条， 后面两个三角使用灰色表示满血的情况。&lt;/p&gt;

&lt;p&gt;项目中我们内置了两个shader, 都是采样顶点色。不同的是VertxSurfShader使用surface shader实现，会计算光照的颜色，而VertxFragShader直接输出顶点色，不参与光照的计算。相对来说VertxFragShader更省一些。&lt;/p&gt;

&lt;p&gt;在VertxSurfShade中我们采样顶点色(appdata_full)来作为vert shader输出的颜色。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;fixed4 _Color;  

struct Input
{
    float4 vertexColor;
};

void vert(inout appdata_full v,out Input o)
{
    UNITY_INITIALIZE_OUTPUT(Input,o);
    o.vertexColor=v.color;
}

void surf (Input IN, inout SurfaceOutput o) 
{
    o.Albedo=IN.vertexColor * _Color;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在VertxFragShader中采样顶点色的使用如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;CGPROGRAM
#pragma vertex vert

#pragma fragment frag

#include &quot;UnityCG.cginc&quot;

fixed4 _Color;  

struct appdata {
    float4 vertex : POSITION;
    fixed4 color : COLOR;
};

struct v2f {
    fixed4 vertexColor : TEXCOORD0;
    float4 vertex : SV_POSITION;
};


v2f vert (appdata v) {
    v2f o;
    o.vertex = UnityObjectToClipPos(v.vertex);
    o.vertexColor = v.color;
    return o;
}

fixed4 frag (v2f i) : SV_Target {
    fixed4 c = i.vertexColor;
    return c * _Color;
}
ENDCG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后设置关联的render组件里关闭反射光线和接收投射阴影。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-hud/hud-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对应的代码实现如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-csharp&quot;&gt;rend.shadowCastingMode = ShadowCastingMode.Off;
rend.receiveShadows = false;
rend.lightProbeUsage = LightProbeUsage.Off;
rend.reflectionProbeUsage = ReflectionProbeUsage.Off;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;绘制文字&quot;&gt;绘制文字&lt;/h2&gt;

&lt;p&gt;绘制文字也不实用nGUI或者uGui使用的组件，而是使用Unity自带的组件TextMesh。我们把TextMesh挂在Hud组件下面，设置好文字大小和对齐方式。对应的代码实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-csharp&quot;&gt;private void CreateText()
{
    GameObject go = new GameObject(&quot;Name&quot;);
    go.transform.SetParent(hud.transform);
    go.transform.rotation = Quaternion.identity;
    go.transform.localScale = 0.1f * Vector3.one;
    go.transform.localPosition = new Vector3(0, 0.5f, 0);
    font = go.AddComponent&amp;lt;TextMesh&amp;gt;();
    font.text = _txt;
    font.fontSize = 36;
    font.alignment = TextAlignment.Center;
    font.anchor = TextAnchor.MiddleCenter;
    font.color = Color.black;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;控制血条进度&quot;&gt;控制血条进度&lt;/h3&gt;

&lt;p&gt;控制血条进度，其实控制的就是顶点1、3、4、6的uv信息。这些顶点的位置在[-x,x] 之间（x=2),我们根据传进来的值在[-2,2]进行差值，算出最终的顶点位置，最后赋值给mesh，来达到控制血条进度的效果。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-csharp&quot;&gt;public void UpdateHud(float v)
{
    float val = Mathf.Lerp(-x, x, 1 - v);
    vertices[1].Set(val, y, z);
    vertices[3].Set(val, -y, z);
    vertices[4] = vertices[1];
    vertices[6] = vertices[3];
    filter.mesh.vertices = vertices;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在Unity中运行HudShow， 在Game视图左上角的GUI-Slider调整进度，我们可以发现对应的血条也跟着发生变化。下面查看Drawcall的信息，运行DCShow，我们创建了大量的hud,并控制不同的血条进度和位置，打开Stats面板，我们看见一共只有四个drawcall，除了摄像机固定的两个drawcall,所有的hud通过合批一共只有2个drawcall。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-hud/hud-3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;好了就这样，拜拜！&lt;/p&gt;

</description>
        <pubDate>Thu, 10 May 2018 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2018/hud/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2018/hud/</guid>
        
        <category>Unity</category>
        
        <category>手机游戏</category>
        
        
      </item>
    
      <item>
        <title>提取视频特征进行人脸识别</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;最近在麦子学院观看美国犹他州立大学的彭亮博士的《机器学习》的视频的系列。不错，就是跟我名字一字之差的彭亮，哈哈，世界巧合的事还真多。不过视频中他演示的《卧虎藏龙》的视频能够人脸识别周润发和章子怡，还真是激起了我的好奇心。不过好在现在互联网这么发达，什么东西只要想学，很多都能找到答案。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;本文所用到的技术：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Keras&lt;/li&gt;
  &lt;li&gt;OpenCV&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所有用到的技术都已上传到github, 读者可以根据自己的需求下载查阅。下载地址：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/huailiang/video-face-recognition&quot;&gt;https://github.com/huailiang/video-face-recognition&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Keras&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;如果说 Tensorflow 或者 Theano 神经网络方面的巨人. 那 Keras 就是站在巨人肩膀上的人. Keras 是一个兼容 Theano 和 Tensorflow 的神经网络高级包, 用他来组件一个神经网络更加快速, 几条语句就搞定了. 而且广泛的兼容性能使 Keras 在 Windows 和 MacOS 或者 Linux 上运行无阻碍.Keras 是建立在 Tensorflow 和 Theano 之上的更高级的神经网络模块, 所以它可以兼容 Windows, Linux 和 MacOS 系统。&lt;/p&gt;

&lt;p&gt;鉴于keras极简的api,本次我们使用keras来搭建一个CNN网络模型，基于tensorflow。之前有一篇文章&lt;a href=&quot;https://huailiang.github.io/2018/03/12/cnn/&quot;&gt;专门介绍CNN&lt;/a&gt;，不懂cnn的同学可以去看看。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-tf/keras.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt; OpenCV&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;OpenCV是一个基于BSD许可（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows、Android和Mac OS操作系统上。它轻量级而且高效——由一系列 C 函数和少量 C++ 类构成，同时提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。&lt;/p&gt;

&lt;p&gt;OpenCV的作用以前还是小看了，最近网易游戏在GDC发布的AirTest自动化测试的工具就是基于OpenCV. 有些人还用OpenCV实现了微信小游戏《跳一跳》自动刷分，虽说里面使用了很多随机的算法，但还是腾讯官方不知道使用了什么算法，居然能检测到作弊，然后给屏蔽掉评分。&lt;/p&gt;

&lt;p&gt;本次利用openCV获取每一帧的所有脸部位置，然后截取脸部部分，传递给我们的CNN模型。cnn模型其实就是一个图片分类器，根据传递过来的图片，经过大量的训练，得到一个分类值（对应的标记label）。获得对应的标记之后，在通过OpenCV gui特性画一个方框，把名字标记在人脸处。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-tf/opencv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;素材获取&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;因为需要训练需要大量的人脸素材，此次项目中所有使用的素材都是来源于UMASS（马萨诸塞大学）的一个对外的网站，这里你可以获取大量的预处理好的关于人脸素材，下载地址：&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;http://vis-www.cs.umass.edu/lfw/&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-tf/face.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于每个人的对应的素材（图片张数）大小不一，我们这里截取了几位数量较多的名人图片的素材来当本地的训练集，比如说美国前任总统George_W_Bush,大概有四五百张。不过其他人好像还是少了点，比如说选择的Laura Bush( George_W_Bush‘s Wife), 犹如素材较少，出现了训练的时候表现很好，测试的时候出错的情况（过拟合-over fit）.&lt;/p&gt;

&lt;p&gt;而且我们专门写了一个python脚本用来删除那些图片数量较少的名人文件夹：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
import shutil

path=&quot;/Users/huailiang.peng/Downloads/lfw_funneled/&quot;
alllist=os.listdir(path)

print len(alllist)

for item in alllist:
	
	fpath=os.path.join(path,item)
	print fpath
	if os.path.isdir(fpath):
		cnt = len(os.listdir(fpath))
		print(&quot;{0} len: {1} &quot;.format(str(item),str(cnt)))
		if cnt&amp;lt;18:
			 shutil.rmtree(fpath)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们的视频素材是从youtube 随便找的一个关于Geoger Bush的演讲视频，貌似清晰度有点问题，不过也不影响我们训练的过程，谁关心呢。&lt;/p&gt;

&lt;p&gt;关于训练集和测试集的构建, 我们从UMASS下载的图片集中最终选取了八位名人的图片做八分类，他们分别是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bill_Clinton&lt;/li&gt;
  &lt;li&gt;George_W_Bush&lt;/li&gt;
  &lt;li&gt;Gerhard_Schroeder&lt;/li&gt;
  &lt;li&gt;Junichiro_Koizumi&lt;/li&gt;
  &lt;li&gt;Laura_Bush&lt;/li&gt;
  &lt;li&gt;Serena_Williams&lt;/li&gt;
  &lt;li&gt;Tony_Blair&lt;/li&gt;
  &lt;li&gt;Winona_Ryder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;项目按文件路径加载图片集，并根据文件夹的名称划给相应的标签, 代码实现如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;images = []
labels = []
def read_path(path_name):    
    for dir_item in os.listdir(path_name):
        #从初始路径开始叠加，合并成可识别的操作路径
        full_path = os.path.abspath(os.path.join(path_name, dir_item))
        
        if os.path.isdir(full_path):    #如果是文件夹，继续递归调用
            read_path(full_path)
        else:   #文件
            if dir_item.endswith(&#39;.jpg&#39;):
                image = cv2.imread(full_path)                
                image = resize_image(image, IMAGE_SIZE, IMAGE_SIZE)
                
                #放开这个代码，可以看到resize_image()函数的实际调用效果
                #cv2.imwrite(&#39;1.jpg&#39;, image)
                
                images.append(image)   
                # print path_name    
                if path_name.endswith(&quot;Bill_Clinton&quot;):  
                    labels.append(0)
                elif path_name.endswith(&quot;George_W_Bush&quot;):
                    labels.append(1)
                elif path_name.endswith(&quot;Laura_Bush&quot;):
                    labels.append(2)
                elif path_name.endswith(&quot;Gerhard_Schroeder&quot;):
                    labels.append(3)
                elif path_name.endswith(&quot;Junichiro_Koizumi&quot;):
                    labels.append(4)
                elif path_name.endswith(&quot;Serena_Williams&quot;):
                    labels.append(5)
                elif path_name.endswith(&quot;Tony_Blair&quot;):
                    labels.append(6)
                else:
                    labels.append(7)                            
                    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;关于CNN网络的搭建，我们借助Keras的代码一共使用了十七层神经网络，统计一共使用了4次卷积，4次激励层，2次池化层，最后还包含全连接层和Dropout层、分类层。对应的代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;def build_model(self, dataset, nb_classes=8):
        # 构建一个空的网络模型，它是一个线性堆叠模型，各神经网络层会被顺序添加，专业名称为序贯模型或线性堆叠模型
        self.model = Sequential()

        # 以下代码将顺序添加CNN网络需要的各层，一个add就是一个网络层
        self.model.add(Convolution2D(32, 3, 3, border_mode=&#39;same&#39;, input_shape=dataset.input_shape))  # 1 2维卷积层
        self.model.add(Activation(&#39;relu&#39;))  # 2 激活函数层

        self.model.add(Convolution2D(32, 3, 3))  # 3 2维卷积层
        self.model.add(Activation(&#39;relu&#39;))  # 4 激活函数层

        self.model.add(MaxPooling2D(pool_size=(2, 2)))  # 5 池化层
        self.model.add(Dropout(0.25))  # 6 Dropout层

        self.model.add(Convolution2D(64, 3, 3, border_mode=&#39;same&#39;))  # 7  2维卷积层
        self.model.add(Activation(&#39;relu&#39;))  # 8  激活函数层

        self.model.add(Convolution2D(64, 3, 3))  # 9  2维卷积层
        self.model.add(Activation(&#39;relu&#39;))  # 10 激活函数层

        self.model.add(MaxPooling2D(pool_size=(2, 2)))  # 11 池化层
        self.model.add(Dropout(0.25))  # 12 Dropout层

        self.model.add(Flatten())  # 13 Flatten层
        self.model.add(Dense(512))  # 14 Dense层,又被称作全连接层
        self.model.add(Activation(&#39;relu&#39;))  # 15 激活函数层
        self.model.add(Dropout(0.5))  # 16 Dropout层
        self.model.add(Dense(nb_classes))  # 17 Dense层
        self.model.add(Activation(&#39;softmax&#39;))  # 18 分类层，输出最终结果

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们使用 model.summary() 可以明了的看清神经网络的组织方式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-tf/tf40.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Keras使用fit方法拟合模型，关于keras的API使用我们这里就不多介绍了，现在已经对应的中文网站出现了，学习起来应该是无压力的。对应到我们的代码就是:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-py&quot;&gt;
    model.fit(dataset.train_images,
                           dataset.train_labels,
                           batch_size = batch_size,
                           nb_epoch = nb_epoch,
                           validation_data = (dataset.valid_images, dataset.valid_labels),
                           shuffle = True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终学习出来的准确率, 有点欠缺人意，哈哈，准确率只达到了96.4%，不过我想也够用了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-tf/tf43.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后如果我们随即使用一张图片集的图片验证，基本上都是对的。但如果我们从Internet上找一张关于Laura Bush的照片，确实很容易就出错了，毕竟laura的训练图片实在是太少了。&lt;/p&gt;

&lt;p&gt;我们还是使用openCv的方式提取图像，传递给模型，基本上bush是可以是别的，但也存在着误差。代码部分这里就不贴出来了，大家可以下载github工程去查看对应的face_predict_use_keras.py脚本。 在场的观众也可能被误认为是Bush，毕竟在训练的时候我们没有这些观众的图片，在经过CNN输出分类的时候就有可能随机是Bush了。可能计算机认为他们长得比较像吧。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-tf/tf41.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 06 Apr 2018 11:00:00 +0800</pubDate>
        <link>https://huailiang.github.io/blog/2018/videoreg/</link>
        <guid isPermaLink="true">https://huailiang.github.io/blog/2018/videoreg/</guid>
        
        <category>工具</category>
        
        <category>人工智能</category>
        
        <category>强化学习</category>
        
        
      </item>
    
  </channel>
</rss>
