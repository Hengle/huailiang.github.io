---
layout:     post
title:      "手机游戏画面风格转换"
date:       2019-05-02 03:00:00
author:     "Huailiang"
tags:
    - Unity
    - 前端开发
    - Tensorflow
    - 人工智能
    - Python
    - 工具
---


> 如今人工智能大行其道， 其中在图像、影音处理中方法颇多。 本文介绍一种游戏中图像风格转换的例子，训练采用Tensorflow-GAN的方式，运行时在Unity引擎使用compute shader实现了跟tensorflow中一样的前向传播的网络来生成转换后的风格。


## 训练集

我们在python-tensorflow中实现了一种反向传播网络，我们使用Auto-Encoder替代GAN中Generator，用以生成风格化的图像， 而在Discrimator来鉴别图像。


训练中采用的训练集是微软的coco的[dataset][i3], 下载转换风格[图片集][i2].


![](/img/in-post/post-tf/style1.jpeg)


我们在训练Discrimator的时候， 评估损失函数，丢给Discrimator风格化的图片尽量大， coco训练集的图片因为是假的， 我们去使其输出的值尽量小， 通过gennerator的图片，也是假的，我们也使其值也越小。


在训练Genenrator评估其损失函数的时候， generator的图片丢给Discrimator，尽量和真实的风格图片尽量接近，因此我们使其输出的值也要越大。 

这里定义content image的内容的损失，方法采用的如[paper](https://arxiv.org/abs/1807.10201) 提到maxpooling方式。

定义style featur的损失， 方法采用把generator生成的图片和原始输入的style image做差求均值。

```python

# Image loss.
self.img_loss_photo = mse_criterion(
    transformer_block(self.output_photo), transformer_block(self.input_photo))
self.img_loss = self.img_loss_photo

# Features loss.
self.feature_loss_photo = abs_criterion(self.output_photo_features, self.input_photo_features)
self.feature_loss = self.feature_loss_photo

```



## 运行时

在unity中， 我们使用compute shader实现了一套跟TensorFlow中的相同的前向传播网络。 这里只是实现了Encoder和Decoder， 并没有实现Discrimator, 因为只有训练的时候用到了Discrimator.

在网络层中 Batch-normal由于为了求整体的均值和方差， 需要遍历当前层每个深度的layer，需要使用归纳算法-reduce, 使计算效率时间复杂度由n变成logn, 所以设计网络的时候尽量减少了类似的操作，在[CompVis](i4)的设计网络中，decoder使用了九个残差模块， 为了性能我们这里减少到了一个。 参数规模也由大概48M减少到12M， 却实现了类似的效果。

由于受限于compute shader的语法， 我们在定义thread group时候， thread的大小不能超过1024，thread-z不能超过64， group的组成是vec3的格式， 而且需要是32或者64的倍数， 因此我们在设计网络的时候，每一个的layer尽量去靠近这些特性， 致使GPU发挥出最大的性能。 （CS5 group thread个数最多是1024， 而CS4最多支持到512，Apple的平台最多支持到CS4，这里需要注意下）。



## 数据导出

tensorflow 训练数据集有自己的序列化方式，大概是protobuf,  google也提供了一套api, 去获取里面的张量Tensor。

通过训练集之后导出的checkpoint文件大小超过一个G，如果把这么庞大的参数文件导入到unity中所开销的内存空间是无法想象的。

通过遍历checkpoint发现所有的tensor发现， 每一层layer， 网络中的每个参数都对应了两个Adam对象，所以我们写了一套工具，导出数据的时候过滤掉Adam对象，使其大小减小到之前的1/3.

在上面提到，discriamtor只在训练的时候用到，而且其参数规模远超generator, 这里我们在导出的时候也需要对其过滤掉。 

通过上面的操作我们导出的参数规模大概是48M，由于可以去掉预算复杂且效率不高的残差网络模块，参数规模进一步缩小到十几兆。 这还是我们采用float存储的格式， 如果对精度要求不高， 采用half的数据格式 5M-6M之间， 我觉得这个大小都手机平台还是可以接受的。







[i1]: https://github.com/huailiang/nnStyle
[i2]: https://hcicloud.iwr.uni-heidelberg.de/index.php/s/NcJj2oLBTYuT1tf
[i3]: http://mscoco.org
[i4]: https://github.com/CompVis/adaptive-style-transfer